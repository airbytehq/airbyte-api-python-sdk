"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from airbyte import utils
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from typing import Final, Optional, Union

class DestinationS3S3(str, Enum):
    S3 = 's3'

class DestinationS3OutputFormatParquetColumnarStorageCompressionCodec(str, Enum):
    r"""The compression algorithm used to compress data pages."""
    UNCOMPRESSED = 'UNCOMPRESSED'
    SNAPPY = 'SNAPPY'
    GZIP = 'GZIP'
    LZO = 'LZO'
    BROTLI = 'BROTLI'
    LZ4 = 'LZ4'
    ZSTD = 'ZSTD'

class DestinationS3OutputFormatParquetColumnarStorageFormatType(str, Enum):
    PARQUET = 'Parquet'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatParquetColumnarStorage:
    r"""Format of the data output. See <a href=\\"https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema\\">here</a> for more details"""
    block_size_mb: Optional[int] = dataclasses.field(default=128, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('block_size_mb'), 'exclude': lambda f: f is None }})
    r"""This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. Default: 128 MB."""
    compression_codec: Optional[DestinationS3OutputFormatParquetColumnarStorageCompressionCodec] = dataclasses.field(default=DestinationS3OutputFormatParquetColumnarStorageCompressionCodec.UNCOMPRESSED, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_codec'), 'exclude': lambda f: f is None }})
    r"""The compression algorithm used to compress data pages."""
    dictionary_encoding: Optional[bool] = dataclasses.field(default=True, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dictionary_encoding'), 'exclude': lambda f: f is None }})
    r"""Default: true."""
    dictionary_page_size_kb: Optional[int] = dataclasses.field(default=1024, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('dictionary_page_size_kb'), 'exclude': lambda f: f is None }})
    r"""There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. Default: 1024 KB."""
    format_type: Optional[DestinationS3OutputFormatParquetColumnarStorageFormatType] = dataclasses.field(default=DestinationS3OutputFormatParquetColumnarStorageFormatType.PARQUET, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('format_type'), 'exclude': lambda f: f is None }})
    max_padding_size_mb: Optional[int] = dataclasses.field(default=8, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('max_padding_size_mb'), 'exclude': lambda f: f is None }})
    r"""Maximum size allowed as padding to align row groups. This is also the minimum size of a row group. Default: 8 MB."""
    page_size_kb: Optional[int] = dataclasses.field(default=1024, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('page_size_kb'), 'exclude': lambda f: f is None }})
    r"""The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. Default: 1024 KB."""
    


class DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionGZIPCompressionType(str, Enum):
    GZIP = 'GZIP'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionGZIP:
    r"""Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \\".jsonl.gz\\")."""
    compression_type: Optional[DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionGZIPCompressionType] = dataclasses.field(default=DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionGZIPCompressionType.GZIP, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_type'), 'exclude': lambda f: f is None }})
    


class DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionNoCompressionCompressionType(str, Enum):
    NO_COMPRESSION = 'No Compression'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionNoCompression:
    r"""Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \\".jsonl.gz\\")."""
    compression_type: Optional[DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionNoCompressionCompressionType] = dataclasses.field(default=DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionNoCompressionCompressionType.NO_COMPRESSION, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_type'), 'exclude': lambda f: f is None }})
    



@dataclasses.dataclass
class DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompression:
    pass

class DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONFlattening(str, Enum):
    r"""Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details."""
    NO_FLATTENING = 'No flattening'
    ROOT_LEVEL_FLATTENING = 'Root level flattening'

class DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONFormatType(str, Enum):
    JSONL = 'JSONL'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatJSONLinesNewlineDelimitedJSON:
    r"""Format of the data output. See <a href=\\"https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema\\">here</a> for more details"""
    compression: Optional[Union[DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionNoCompression, DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONCompressionGZIP]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression'), 'exclude': lambda f: f is None }})
    r"""Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \\".jsonl.gz\\")."""
    flattening: Optional[DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONFlattening] = dataclasses.field(default=DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONFlattening.NO_FLATTENING, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('flattening'), 'exclude': lambda f: f is None }})
    r"""Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details."""
    format_type: Optional[DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONFormatType] = dataclasses.field(default=DestinationS3OutputFormatJSONLinesNewlineDelimitedJSONFormatType.JSONL, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('format_type'), 'exclude': lambda f: f is None }})
    


class DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionGZIPCompressionType(str, Enum):
    GZIP = 'GZIP'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionGZIP:
    r"""Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \\".csv.gz\\")."""
    compression_type: Optional[DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionGZIPCompressionType] = dataclasses.field(default=DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionGZIPCompressionType.GZIP, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_type'), 'exclude': lambda f: f is None }})
    


class DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionNoCompressionCompressionType(str, Enum):
    NO_COMPRESSION = 'No Compression'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionNoCompression:
    r"""Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \\".csv.gz\\")."""
    compression_type: Optional[DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionNoCompressionCompressionType] = dataclasses.field(default=DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionNoCompressionCompressionType.NO_COMPRESSION, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_type'), 'exclude': lambda f: f is None }})
    



@dataclasses.dataclass
class DestinationS3OutputFormatCSVCommaSeparatedValuesCompression:
    pass

class DestinationS3OutputFormatCSVCommaSeparatedValuesFlattening(str, Enum):
    r"""Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details."""
    NO_FLATTENING = 'No flattening'
    ROOT_LEVEL_FLATTENING = 'Root level flattening'

class DestinationS3OutputFormatCSVCommaSeparatedValuesFormatType(str, Enum):
    CSV = 'CSV'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatCSVCommaSeparatedValues:
    r"""Format of the data output. See <a href=\\"https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema\\">here</a> for more details"""
    compression: Optional[Union[DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionNoCompression, DestinationS3OutputFormatCSVCommaSeparatedValuesCompressionGZIP]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression'), 'exclude': lambda f: f is None }})
    r"""Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \\".csv.gz\\")."""
    flattening: Optional[DestinationS3OutputFormatCSVCommaSeparatedValuesFlattening] = dataclasses.field(default=DestinationS3OutputFormatCSVCommaSeparatedValuesFlattening.NO_FLATTENING, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('flattening'), 'exclude': lambda f: f is None }})
    r"""Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details."""
    format_type: Optional[DestinationS3OutputFormatCSVCommaSeparatedValuesFormatType] = dataclasses.field(default=DestinationS3OutputFormatCSVCommaSeparatedValuesFormatType.CSV, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('format_type'), 'exclude': lambda f: f is None }})
    


class DestinationS3OutputFormatAvroApacheAvroCompressionCodecSnappyCodec(str, Enum):
    SNAPPY = 'snappy'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatAvroApacheAvroCompressionCodecSnappy:
    r"""The compression algorithm used to compress data. Default to no compression."""
    codec: Optional[DestinationS3OutputFormatAvroApacheAvroCompressionCodecSnappyCodec] = dataclasses.field(default=DestinationS3OutputFormatAvroApacheAvroCompressionCodecSnappyCodec.SNAPPY, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('codec'), 'exclude': lambda f: f is None }})
    


class DestinationS3OutputFormatAvroApacheAvroCompressionCodecZstandardCodec(str, Enum):
    ZSTANDARD = 'zstandard'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatAvroApacheAvroCompressionCodecZstandard:
    r"""The compression algorithm used to compress data. Default to no compression."""
    codec: Optional[DestinationS3OutputFormatAvroApacheAvroCompressionCodecZstandardCodec] = dataclasses.field(default=DestinationS3OutputFormatAvroApacheAvroCompressionCodecZstandardCodec.ZSTANDARD, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('codec'), 'exclude': lambda f: f is None }})
    compression_level: Optional[int] = dataclasses.field(default=3, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_level'), 'exclude': lambda f: f is None }})
    r"""Negative levels are 'fast' modes akin to lz4 or snappy, levels above 9 are generally for archival purposes, and levels above 18 use a lot of memory."""
    include_checksum: Optional[bool] = dataclasses.field(default=False, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('include_checksum'), 'exclude': lambda f: f is None }})
    r"""If true, include a checksum with each data block."""
    


class DestinationS3OutputFormatAvroApacheAvroCompressionCodecXzCodec(str, Enum):
    XZ = 'xz'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatAvroApacheAvroCompressionCodecXz:
    r"""The compression algorithm used to compress data. Default to no compression."""
    codec: Optional[DestinationS3OutputFormatAvroApacheAvroCompressionCodecXzCodec] = dataclasses.field(default=DestinationS3OutputFormatAvroApacheAvroCompressionCodecXzCodec.XZ, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('codec'), 'exclude': lambda f: f is None }})
    compression_level: Optional[int] = dataclasses.field(default=6, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_level'), 'exclude': lambda f: f is None }})
    r"""See <a href=\\"https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/compressors/xz/XZCompressorOutputStream.html#XZCompressorOutputStream-java.io.OutputStream-int-\\">here</a> for details."""
    


class DestinationS3OutputFormatAvroApacheAvroCompressionCodecBzip2Codec(str, Enum):
    BZIP2 = 'bzip2'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatAvroApacheAvroCompressionCodecBzip2:
    r"""The compression algorithm used to compress data. Default to no compression."""
    codec: Optional[DestinationS3OutputFormatAvroApacheAvroCompressionCodecBzip2Codec] = dataclasses.field(default=DestinationS3OutputFormatAvroApacheAvroCompressionCodecBzip2Codec.BZIP2, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('codec'), 'exclude': lambda f: f is None }})
    


class DestinationS3OutputFormatAvroApacheAvroCompressionCodecDeflateCodec(str, Enum):
    DEFLATE = 'Deflate'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatAvroApacheAvroCompressionCodecDeflate:
    r"""The compression algorithm used to compress data. Default to no compression."""
    codec: Optional[DestinationS3OutputFormatAvroApacheAvroCompressionCodecDeflateCodec] = dataclasses.field(default=DestinationS3OutputFormatAvroApacheAvroCompressionCodecDeflateCodec.DEFLATE, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('codec'), 'exclude': lambda f: f is None }})
    compression_level: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_level'), 'exclude': lambda f: f is None }})
    r"""0: no compression & fastest, 9: best compression & slowest."""
    


class DestinationS3OutputFormatAvroApacheAvroCompressionCodecNoCompressionCodec(str, Enum):
    NO_COMPRESSION = 'no compression'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatAvroApacheAvroCompressionCodecNoCompression:
    r"""The compression algorithm used to compress data. Default to no compression."""
    codec: Optional[DestinationS3OutputFormatAvroApacheAvroCompressionCodecNoCompressionCodec] = dataclasses.field(default=DestinationS3OutputFormatAvroApacheAvroCompressionCodecNoCompressionCodec.NO_COMPRESSION, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('codec'), 'exclude': lambda f: f is None }})
    



@dataclasses.dataclass
class DestinationS3OutputFormatAvroApacheAvroCompressionCodec:
    pass

class DestinationS3OutputFormatAvroApacheAvroFormatType(str, Enum):
    AVRO = 'Avro'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3OutputFormatAvroApacheAvro:
    r"""Format of the data output. See <a href=\\"https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema\\">here</a> for more details"""
    compression_codec: Union[DestinationS3OutputFormatAvroApacheAvroCompressionCodecNoCompression, DestinationS3OutputFormatAvroApacheAvroCompressionCodecDeflate, DestinationS3OutputFormatAvroApacheAvroCompressionCodecBzip2, DestinationS3OutputFormatAvroApacheAvroCompressionCodecXz, DestinationS3OutputFormatAvroApacheAvroCompressionCodecZstandard, DestinationS3OutputFormatAvroApacheAvroCompressionCodecSnappy] = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_codec') }})
    r"""The compression algorithm used to compress data. Default to no compression."""
    format_type: Optional[DestinationS3OutputFormatAvroApacheAvroFormatType] = dataclasses.field(default=DestinationS3OutputFormatAvroApacheAvroFormatType.AVRO, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('format_type'), 'exclude': lambda f: f is None }})
    



@dataclasses.dataclass
class DestinationS3OutputFormat:
    pass

class DestinationS3S3BucketRegion(str, Enum):
    r"""The region of the S3 bucket. See <a href=\\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\\">here</a> for all region codes."""
    UNKNOWN = ''
    US_EAST_1 = 'us-east-1'
    US_EAST_2 = 'us-east-2'
    US_WEST_1 = 'us-west-1'
    US_WEST_2 = 'us-west-2'
    AF_SOUTH_1 = 'af-south-1'
    AP_EAST_1 = 'ap-east-1'
    AP_SOUTH_1 = 'ap-south-1'
    AP_NORTHEAST_1 = 'ap-northeast-1'
    AP_NORTHEAST_2 = 'ap-northeast-2'
    AP_NORTHEAST_3 = 'ap-northeast-3'
    AP_SOUTHEAST_1 = 'ap-southeast-1'
    AP_SOUTHEAST_2 = 'ap-southeast-2'
    CA_CENTRAL_1 = 'ca-central-1'
    CN_NORTH_1 = 'cn-north-1'
    CN_NORTHWEST_1 = 'cn-northwest-1'
    EU_CENTRAL_1 = 'eu-central-1'
    EU_NORTH_1 = 'eu-north-1'
    EU_SOUTH_1 = 'eu-south-1'
    EU_WEST_1 = 'eu-west-1'
    EU_WEST_2 = 'eu-west-2'
    EU_WEST_3 = 'eu-west-3'
    SA_EAST_1 = 'sa-east-1'
    ME_SOUTH_1 = 'me-south-1'
    US_GOV_EAST_1 = 'us-gov-east-1'
    US_GOV_WEST_1 = 'us-gov-west-1'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationS3:
    r"""The values required to configure the destination."""
    format: Union[DestinationS3OutputFormatAvroApacheAvro, DestinationS3OutputFormatCSVCommaSeparatedValues, DestinationS3OutputFormatJSONLinesNewlineDelimitedJSON, DestinationS3OutputFormatParquetColumnarStorage] = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('format') }})
    r"""Format of the data output. See <a href=\\"https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema\\">here</a> for more details"""
    s3_bucket_name: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_bucket_name') }})
    r"""The name of the S3 bucket. Read more <a href=\\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html\\">here</a>."""
    s3_bucket_path: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_bucket_path') }})
    r"""Directory under the S3 bucket where data will be written. Read more <a href=\\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=to%20format%20the-,bucket%20path,-%3A\\">here</a>"""
    DESTINATION_TYPE: Final[DestinationS3S3] = dataclasses.field(default=DestinationS3S3.S3, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('destinationType') }})
    access_key_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('access_key_id'), 'exclude': lambda f: f is None }})
    r"""The access key ID to access the S3 bucket. Airbyte requires Read and Write permissions to the given bucket. Read more <a href=\\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\\">here</a>."""
    file_name_pattern: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('file_name_pattern'), 'exclude': lambda f: f is None }})
    r"""The pattern allows you to set the file-name format for the S3 staging file(s)"""
    s3_bucket_region: Optional[DestinationS3S3BucketRegion] = dataclasses.field(default=DestinationS3S3BucketRegion.UNKNOWN, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_bucket_region'), 'exclude': lambda f: f is None }})
    r"""The region of the S3 bucket. See <a href=\\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\\">here</a> for all region codes."""
    s3_endpoint: Optional[str] = dataclasses.field(default='', metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_endpoint'), 'exclude': lambda f: f is None }})
    r"""Your S3 endpoint url. Read more <a href=\\"https://docs.aws.amazon.com/general/latest/gr/s3.html#:~:text=Service%20endpoints-,Amazon%20S3%20endpoints,-When%20you%20use\\">here</a>"""
    s3_path_format: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('s3_path_format'), 'exclude': lambda f: f is None }})
    r"""Format string on how data will be organized inside the S3 bucket directory. Read more <a href=\\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=The%20full%20path%20of%20the%20output%20data%20with%20the%20default%20S3%20path%20format\\">here</a>"""
    secret_access_key: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('secret_access_key'), 'exclude': lambda f: f is None }})
    r"""The corresponding secret to the access key ID. Read more <a href=\\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\\">here</a>"""
    

