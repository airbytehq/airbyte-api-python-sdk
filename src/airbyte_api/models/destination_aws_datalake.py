"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from airbyte_api import utils
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from typing import Final, Optional, Union


class DestinationAwsDatalakeCredentialsTitle(str, Enum):
    r"""Name of the credentials"""
    IAM_USER = 'IAM User'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class IAMUser:
    aws_access_key_id: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('aws_access_key_id') }})
    r"""AWS User Access Key Id"""
    aws_secret_access_key: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('aws_secret_access_key') }})
    r"""Secret Access Key"""
    CREDENTIALS_TITLE: Final[Optional[DestinationAwsDatalakeCredentialsTitle]] = dataclasses.field(default=DestinationAwsDatalakeCredentialsTitle.IAM_USER, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('credentials_title'), 'exclude': lambda f: f is None }})
    r"""Name of the credentials"""
    



class CredentialsTitle(str, Enum):
    r"""Name of the credentials"""
    IAM_ROLE = 'IAM Role'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class IAMRole:
    role_arn: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('role_arn') }})
    r"""Will assume this role to write data to s3"""
    CREDENTIALS_TITLE: Final[Optional[CredentialsTitle]] = dataclasses.field(default=CredentialsTitle.IAM_ROLE, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('credentials_title'), 'exclude': lambda f: f is None }})
    r"""Name of the credentials"""
    



class AwsDatalake(str, Enum):
    AWS_DATALAKE = 'aws-datalake'


class DestinationAwsDatalakeCompressionCodecOptional(str, Enum):
    r"""The compression algorithm used to compress data."""
    UNCOMPRESSED = 'UNCOMPRESSED'
    SNAPPY = 'SNAPPY'
    GZIP = 'GZIP'
    ZSTD = 'ZSTD'


class DestinationAwsDatalakeFormatTypeWildcard(str, Enum):
    PARQUET = 'Parquet'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class ParquetColumnarStorage:
    compression_codec: Optional[DestinationAwsDatalakeCompressionCodecOptional] = dataclasses.field(default=DestinationAwsDatalakeCompressionCodecOptional.SNAPPY, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_codec'), 'exclude': lambda f: f is None }})
    r"""The compression algorithm used to compress data."""
    format_type: Optional[DestinationAwsDatalakeFormatTypeWildcard] = dataclasses.field(default=DestinationAwsDatalakeFormatTypeWildcard.PARQUET, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('format_type'), 'exclude': lambda f: f is None }})
    



class CompressionCodecOptional(str, Enum):
    r"""The compression algorithm used to compress data."""
    UNCOMPRESSED = 'UNCOMPRESSED'
    GZIP = 'GZIP'


class FormatTypeWildcard(str, Enum):
    JSONL = 'JSONL'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class JSONLinesNewlineDelimitedJSON:
    compression_codec: Optional[CompressionCodecOptional] = dataclasses.field(default=CompressionCodecOptional.UNCOMPRESSED, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compression_codec'), 'exclude': lambda f: f is None }})
    r"""The compression algorithm used to compress data."""
    format_type: Optional[FormatTypeWildcard] = dataclasses.field(default=FormatTypeWildcard.JSONL, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('format_type'), 'exclude': lambda f: f is None }})
    



class ChooseHowToPartitionData(str, Enum):
    r"""Partition data by cursor fields when a cursor field is a date"""
    NO_PARTITIONING = 'NO PARTITIONING'
    DATE = 'DATE'
    YEAR = 'YEAR'
    MONTH = 'MONTH'
    DAY = 'DAY'
    YEAR_MONTH = 'YEAR/MONTH'
    YEAR_MONTH_DAY = 'YEAR/MONTH/DAY'


class S3BucketRegion(str, Enum):
    r"""The region of the S3 bucket. See <a href=\\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\\">here</a> for all region codes."""
    UNKNOWN = ''
    AF_SOUTH_1 = 'af-south-1'
    AP_EAST_1 = 'ap-east-1'
    AP_NORTHEAST_1 = 'ap-northeast-1'
    AP_NORTHEAST_2 = 'ap-northeast-2'
    AP_NORTHEAST_3 = 'ap-northeast-3'
    AP_SOUTH_1 = 'ap-south-1'
    AP_SOUTH_2 = 'ap-south-2'
    AP_SOUTHEAST_1 = 'ap-southeast-1'
    AP_SOUTHEAST_2 = 'ap-southeast-2'
    AP_SOUTHEAST_3 = 'ap-southeast-3'
    AP_SOUTHEAST_4 = 'ap-southeast-4'
    CA_CENTRAL_1 = 'ca-central-1'
    CA_WEST_1 = 'ca-west-1'
    CN_NORTH_1 = 'cn-north-1'
    CN_NORTHWEST_1 = 'cn-northwest-1'
    EU_CENTRAL_1 = 'eu-central-1'
    EU_CENTRAL_2 = 'eu-central-2'
    EU_NORTH_1 = 'eu-north-1'
    EU_SOUTH_1 = 'eu-south-1'
    EU_SOUTH_2 = 'eu-south-2'
    EU_WEST_1 = 'eu-west-1'
    EU_WEST_2 = 'eu-west-2'
    EU_WEST_3 = 'eu-west-3'
    IL_CENTRAL_1 = 'il-central-1'
    ME_CENTRAL_1 = 'me-central-1'
    ME_SOUTH_1 = 'me-south-1'
    SA_EAST_1 = 'sa-east-1'
    US_EAST_1 = 'us-east-1'
    US_EAST_2 = 'us-east-2'
    US_GOV_EAST_1 = 'us-gov-east-1'
    US_GOV_WEST_1 = 'us-gov-west-1'
    US_WEST_1 = 'us-west-1'
    US_WEST_2 = 'us-west-2'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DestinationAwsDatalake:
    bucket_name: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bucket_name') }})
    r"""The name of the S3 bucket. Read more <a href=\\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html\\">here</a>."""
    credentials: AuthenticationMode = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('credentials') }})
    r"""Choose How to Authenticate to AWS."""
    lakeformation_database_name: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lakeformation_database_name') }})
    r"""The default database this destination will use to create tables in per stream. Can be changed per connection by customizing the namespace."""
    aws_account_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('aws_account_id'), 'exclude': lambda f: f is None }})
    r"""target aws account id"""
    bucket_prefix: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('bucket_prefix'), 'exclude': lambda f: f is None }})
    r"""S3 prefix"""
    DESTINATION_TYPE: Final[AwsDatalake] = dataclasses.field(default=AwsDatalake.AWS_DATALAKE, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('destinationType') }})
    format: Optional[OutputFormatWildcard] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('format'), 'exclude': lambda f: f is None }})
    r"""Format of the data output."""
    glue_catalog_float_as_decimal: Optional[bool] = dataclasses.field(default=False, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('glue_catalog_float_as_decimal'), 'exclude': lambda f: f is None }})
    r"""Cast float/double as decimal(38,18). This can help achieve higher accuracy and represent numbers correctly as received from the source."""
    lakeformation_database_default_tag_key: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lakeformation_database_default_tag_key'), 'exclude': lambda f: f is None }})
    r"""Add a default tag key to databases created by this destination"""
    lakeformation_database_default_tag_values: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lakeformation_database_default_tag_values'), 'exclude': lambda f: f is None }})
    r"""Add default values for the `Tag Key` to databases created by this destination. Comma separate for multiple values."""
    lakeformation_governed_tables: Optional[bool] = dataclasses.field(default=False, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('lakeformation_governed_tables'), 'exclude': lambda f: f is None }})
    r"""Whether to create tables as LF governed tables."""
    partitioning: Optional[ChooseHowToPartitionData] = dataclasses.field(default=ChooseHowToPartitionData.NO_PARTITIONING, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('partitioning'), 'exclude': lambda f: f is None }})
    r"""Partition data by cursor fields when a cursor field is a date"""
    region: Optional[S3BucketRegion] = dataclasses.field(default=S3BucketRegion.UNKNOWN, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('region'), 'exclude': lambda f: f is None }})
    r"""The region of the S3 bucket. See <a href=\\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\\">here</a> for all region codes."""
    


AuthenticationMode = Union[IAMRole, IAMUser]

OutputFormatWildcard = Union[JSONLinesNewlineDelimitedJSON, ParquetColumnarStorage]
